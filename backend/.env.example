# Backend environment variables

# CORS origins for HTTP requests
ALLOWED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000,http://localhost:5173,http://127.0.0.1:5173

# LLM provider selection for WebSocket chat:
# Options: openrouter | ollama | sambanova | anakin
LLM_PROVIDER=openrouter

# Optional toggles (alternatively to LLM_PROVIDER)
# USE_OLLAMA=true
# USE_ANAKIN=true

# ----------------------------
# OpenRouter (default provider)
# ----------------------------
# Required if LLM_PROVIDER=openrouter
# Sign up at https://openrouter.ai and create an API key
OPENROUTER_API_KEY=your_openrouter_key

# Model to use on OpenRouter, e.g.:
# - anthropic/claude-3.7-sonnet:thinking
# - anthropic/claude-3.5-sonnet
# - openai/gpt-4o-mini
# - google/gemini-2.5-pro
OPENROUTER_API_MODEL=anthropic/claude-3.5-sonnet

# Optional branding headers (helps model marketplaces attribute usage)
# OPENROUTER_SITE_URL=https://your.site
# OPENROUTER_SITE_TITLE=Policy Proof

# ----------------------------
# Ollama (local)
# ----------------------------
# If LLM_PROVIDER=ollama (or USE_OLLAMA=true)
# Run: `ollama serve` and pull a model: `ollama pull llama3`
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama3

# ----------------------------
# SambaNova
# ----------------------------
# If LLM_PROVIDER=sambanova
# Create an API key at https://cloud.sambanova.ai
SAMBANOVA_API_KEY=
# Example model: "Llama-4-Scout-17B-16E-Instruct" (check provider catalog)
SAMBANOVA_MODEL=Llama-4-Scout-17B-16E-Instruct

# ----------------------------
# Anakin
# ----------------------------
# If LLM_PROVIDER=anakin (or USE_ANAKIN=true)
ANAKIN_API_KEY=
# Your chatbot/app ID from Anakin
ANAKIN_APP_ID=
# Optional version pin
ANAKIN_API_VERSION=2024-05-06
